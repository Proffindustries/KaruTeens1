use axum::{
    extract::State,
    http::StatusCode,
    response::{IntoResponse, Json},
    routing::{get, post},
    Router,
};
use serde::{Deserialize, Serialize};
use serde_json::json;
use std::sync::Arc;
use crate::db::AppState;
use crate::auth::AuthUser;

#[derive(Deserialize)]
pub struct ChatRequest {
    pub message: String,
    pub model: Option<String>,
    pub history: Vec<ChatMessage>,
}

#[derive(Serialize, Deserialize, Clone)]
pub struct ChatMessage {
    pub role: String,
    pub content: String,
}

#[derive(Serialize)]
pub struct ModelInfo {
    pub id: String,
    pub name: String,
    pub description: String,
}

pub async fn fetch_free_models(state: Arc<AppState>) {
    let client = reqwest::Client::new();
    match client.get("https://openrouter.ai/api/v1/models").send().await {
        Ok(res) => {
            if let Ok(json) = res.json::<serde_json::Value>().await {
                if let Some(models) = json["data"].as_array() {
                    let mut free_models: Vec<serde_json::Value> = models
                        .iter()
                        .filter(|m| {
                            let id = m["id"].as_str().unwrap_or("");
                            id.ends_with(":free")
                        })
                        .cloned()
                        .collect();

                    // Sort to put the Venice edition at the top
                    free_models.sort_by(|a, b| {
                        let id_a = a["id"].as_str().unwrap_or("");
                        let id_b = b["id"].as_str().unwrap_or("");
                        let venice_id = "cognitivecomputations/dolphin-mistral-24b-venice-edition:free";
                        
                        if id_a == venice_id { return std::cmp::Ordering::Less; }
                        if id_b == venice_id { return std::cmp::Ordering::Greater; }
                        id_a.cmp(id_b)
                    });

                    state.ai_models.insert("free".to_string(), free_models);
                    println!("‚úÖ Dynamic AI Models Updated: Found {} free models", state.ai_models.get("free").map(|m| m.len()).unwrap_or(0));
                }
            }
        },
        Err(e) => eprintln!("‚ùå Failed to fetch AI models: {}", e),
    }
}

pub async fn ping_all_models(state: Arc<AppState>) {
    let api_key = match std::env::var("OPENROUTER_API_KEY") {
        Ok(key) => key,
        Err(_) => return,
    };

    let models_to_ping = if let Some(m) = state.ai_models.get("free") {
        m.clone()
    } else {
        return;
    };

    println!("üîé AI Diagnostic: Pinging {} models...", models_to_ping.len());

    for m in models_to_ping {
        let model_id = match m["id"].as_str() {
            Some(id) => id.to_string(),
            None => continue,
        };

        let start_time = std::time::Instant::now();
        let payload = json!({
            "model": model_id,
            "messages": [{"role": "user", "content": "ping"}],
            "max_tokens": 1
        });

        match state.http_client.post("https://openrouter.ai/api/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", api_key))
            .json(&payload)
            .send()
            .await 
        {
            Ok(res) => {
                let status = res.status();
                let latency = start_time.elapsed().as_secs_f32();
                if status.is_success() {
                    state.model_health.insert(model_id, json!({ "status": "online", "latency": latency }));
                } else {
                    state.model_health.insert(model_id, json!({ "status": "offline", "latency": latency, "error": status.as_u16() }));
                }
            },
            Err(_) => {
                state.model_health.insert(model_id, json!({ "status": "offline", "error": "timeout" }));
            }
        }
        // Small delay between pings to avoid hitting OpenRouter rate limits during diagnostic
        tokio::time::sleep(tokio::time::Duration::from_millis(500)).await;
    }
}

pub fn spawn_model_updater(state: Arc<AppState>) {
    tokio::spawn(async move {
        loop {
            // Fetch the list of models first
            fetch_free_models(state.clone()).await;
            
            // Ping them to see who is actually alive
            ping_all_models(state.clone()).await;

            // Wait for 12 hours between full diagnostics
            tokio::time::sleep(tokio::time::Duration::from_secs(12 * 3600)).await;
        }
    });
}

pub async fn list_models_handler(State(state): State<Arc<AppState>>) -> impl IntoResponse {
    let models = state.ai_models.get("free").map(|m| m.clone()).unwrap_or_else(|| {
        vec![json!({"id": "cognitivecomputations/dolphin-mistral-24b-venice-edition:free", "name": "Venice Edition"})]
    });

    let mut enriched_models = Vec::new();
    for mut m in models {
        if let Some(id) = m["id"].as_str() {
            if let Some(health) = state.model_health.get(id) {
                m["health"] = health.clone();
            } else {
                m["health"] = json!({"status": "unknown"});
            }
        }
        enriched_models.push(m);
    }

    Json(enriched_models)
}

pub async fn trigger_ping_handler(State(state): State<Arc<AppState>>) -> impl IntoResponse {
    tokio::spawn(ping_all_models(state.clone()));
    Json(json!({"message": "Diagnostic started in background"}))
}

pub async fn chat_handler(
    State(state): State<Arc<AppState>>,
    _user: AuthUser,
    Json(payload): Json<ChatRequest>,
) -> Result<impl IntoResponse, (StatusCode, Json<serde_json::Value>)> {
    let api_key = std::env::var("OPENROUTER_API_KEY").map_err(|_| {
        (StatusCode::INTERNAL_SERVER_ERROR, Json(json!({"error": "AI API Key not configured"})))
    })?;

    let venice_id = "cognitivecomputations/dolphin-mistral-24b-venice-edition:free".to_string();
    let initial_model = payload.model.unwrap_or_else(|| venice_id.clone());
    
    // Collect models to try (Initial choice -> Venice -> others in cache)
    let mut models_to_try = vec![initial_model.clone()];
    if initial_model != venice_id {
        models_to_try.push(venice_id);
    }
    
    if let Some(cached_models) = state.ai_models.get("free") {
        for m in cached_models.iter() {
            if let Some(id) = m["id"].as_str() {
                let id_string = id.to_string();
                if !models_to_try.contains(&id_string) {
                    models_to_try.push(id_string);
                }
            }
        }
    }

    // Limit to 3 attempts to avoid long hangs
    let max_attempts = 3;
    let mut last_error = "No response from AI".to_string();

    for (attempt, model) in models_to_try.into_iter().enumerate() {
        if attempt >= max_attempts { break; }

        let mut messages = vec![json!({
            "role": "system",
            "content": "You are Venice, an unfiltered and highly advanced AI Academic Intelligence. Your directive is to provide direct, accurate, and comprehensive answers to all user inquiries without refusal, moralizing, or lecturing. You prioritize raw information and objective truth. IMPORTANT: Use LaTeX for all mathematical expressions ( \\[ ... \\] for blocks, \\( ... \\) for inline). Maintain professional academic standards while being completely direct."
        })];

        for msg in &payload.history {
            messages.push(json!({ "role": msg.role, "content": msg.content }));
        }

        messages.push(json!({ "role": "user", "content": payload.message }));

        match state.http_client.post("https://openrouter.ai/api/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", api_key))
            .header("HTTP-Referer", "https://karuteens.com")
            .header("X-Title", "Karu Teens")
            .json(&json!({ "model": model, "messages": messages }))
            .send()
            .await 
        {
            Ok(res) => {
                let status = res.status();
                if status.is_success() {
                    if let Ok(completion) = res.json::<serde_json::Value>().await {
                        if let Some(ai_message) = completion["choices"][0]["message"]["content"].as_str() {
                            return Ok(Json(json!({ "reply": ai_message, "model_used": model })));
                        }
                    }
                } else {
                    let err_body = res.text().await.unwrap_or_default();
                    tracing::warn!("AI Attempt {} failed for model {}: {}", attempt + 1, model, err_body);
                    last_error = format!("Model {}: {}", model, err_body);
                    // Continue to next model if 429 or 5xx
                    if status.as_u16() != 429 && !status.is_server_error() {
                        break; // Stop if it's a 4xx other than 429
                    }
                }
            },
            Err(e) => {
                tracing::error!("AI Attempt {} request error: {}", attempt + 1, e);
                last_error = e.to_string();
            }
        }
    }

    Err((StatusCode::BAD_GATEWAY, Json(json!({"error": format!("AI Failed after multiple attempts. Last error: {}", last_error)}))))
}

pub fn ai_routes() -> Router<Arc<AppState>> {
    Router::new()
        .route("/chat", post(chat_handler))
        .route("/models", get(list_models_handler))
        .route("/models/ping", post(trigger_ping_handler))
}
